3_EMA: 1.33MB  ODS=0.776
4_SEGate: 0.99MB  ODS=0.772
5_GEMA: 1.37MB  ODS=?
2_MSPA: 1.41MB  ODS=0.777

9和90两个试图对CPDC动刀的模块效果都很烂，感觉不能动CPDC本身
不同的注意力机制对模型大小的影响较小，而使用门控的SE体积显著小，暂时认为是用于cat后融合的3x3卷积导致了参数量的增加
因此考虑在2_MSPA中将其替换为1x1卷积，即91_MSPA_Lightfuse
91_MSPA_Lightfuse的参数从1.41MB降到0.80MB，显然想法是对的...但是担心融合效果会因此大幅度降低
MSPA的存在导致训练速度显著降低，需要更加高效的注意力机制


有点尴尬，改进后的门控se还是打不过baseline，但是baseline复现不了...重新跑一遍，希望能出结果
base:源代码
MSPA:并联了MSPA模块
MSPA_up:MSPA的基础上将上采样换成了Dysample

以上是在实验室服务器跑的，保证准确，下面是需要租显卡跑验证是否有优化效果的：
decoder: 将base的decoder替换为基于shufflenet思想改进的decoder

考虑使用模块75b MFM的改版，取代3*3实现融合
还有62a CFF改版



已完成：
MSPA 2
base base
MSPA_up 92
MSPA_Lightfusion 94
91 0.769 降低 1x1卷积直接替换掉了3x3卷积 91误用了upblock，可能有误，但以后再说
93 0.775 提高 将decoder的3*3卷积替换为FastConvList很有用
95 0.768 降低 shuffle decoder废物

结论：
源代码依然只能跑到770，跑不到775
MSPA能提升性能，但是只到771，不够
Lightfusion里面取代3*3卷积的新融合方式成功了，提升到774
MSPA_up无法理解为什么效果这么好达到776
将decoder的3*3卷积替换为FastConvList很有用，提高到775
shuffle decoder废物
mixblock的融合不能直接用1x1

Todo：
MFM和DFF两种融合模块的验证 96 分别在16和15号租的显卡上跑着呢
(已完成)测试效果： 95_base_decoder2 14号显卡 是将decoder换成3个shufflenet思想的模块串联 ？疑似有两版文件，暂停下载，稍后重新从服务器下载试试
(已完成)测试效果： 91_MSPA_Lightfuse_wrongupdown-out30 是将3*3直接改为1*1，但是上采样误用了upblock
(已完成)测试效果： 93_MSPA_decoder-out30 将decoder中的3*3卷积改为了FastConvList（2层），上下采样都没弄错

寻找baseline效果不佳的原因

目前创新：
1- 核心块，对CPDC并联后融合被验证可行
2- Dysample?
3- MSEM在取消掉attn后勉强算是? （不用强调）
    4- 轻量化的融合模块 （x） 是废物
5- 93的修改后decoder
6- 97的before decoder

data:
baseline:0.770 0.786
baseline+MSPA:0.777 0.793
baseline+MSPA(重新测的):0.771 0.789
baseline+MSPA+New_upsample:0.776 0.791
baseline+MSPA+New_decoder:0.775 0.791
baseline+MSPA+attn_before_decoder:0.772 0.789


