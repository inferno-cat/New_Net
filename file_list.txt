3_EMA: 1.33MB  ODS=0.776
4_SEGate: 0.99MB  ODS=0.772
5_GEMA: 1.37MB  ODS=?
2_MSPA: 1.41MB  ODS=0.777

9和90两个试图对CPDC动刀的模块效果都很烂，感觉不能动CPDC本身
不同的注意力机制对模型大小的影响较小，而使用门控的SE体积显著小，暂时认为是用于cat后融合的3x3卷积导致了参数量的增加
因此考虑在2_MSPA中将其替换为1x1卷积，即91_MSPA_Lightfuse
91_MSPA_Lightfuse的参数从1.41MB降到0.80MB，显然想法是对的...但是担心融合效果会因此大幅度降低
MSPA的存在导致训练速度显著降低，需要更加高效的注意力机制
